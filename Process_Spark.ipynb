{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e79b6d-5de9-48f4-a71e-b96dd6b04ff4",
   "metadata": {},
   "source": [
    "# Install lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e40d6c5-1c97-40a7-b7af-05bdbea703fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /Users/pirayan/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyspark in /Users/pirayan/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/pirayan/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2616d199-76ab-45c4-8a20-3c320e0783b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to print the json with the indent=4\n",
    "def jprint(data):\n",
    "  print(json.dumps(data,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6c86c-9126-4d3c-9c99-c66d7741b0bb",
   "metadata": {},
   "source": [
    "# Spark for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a18935d5-f30d-452c-9423-53d9e84b17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+------------+-------------+--------+------------------------+\n",
      "|id       |publicDate|source|coAuthorship|citationCount|refCount|Class                   |\n",
      "+---------+----------+------+------------+-------------+--------+------------------------+\n",
      "|202200000|31/12/2022|1     |3           |2            |44      |[BIOC, CENG, ENVI, ENGI]|\n",
      "|202200001|31/12/2022|1     |7           |5            |39      |[BIOC]                  |\n",
      "|202200002|31/12/2022|1     |6           |7            |51      |[BIOC]                  |\n",
      "|202200003|31/12/2022|1     |5           |14           |50      |[CHEM, CENG, ENGI]      |\n",
      "|202200004|30/12/2022|1     |6           |2            |65      |[PHYS, MATE]            |\n",
      "|202200005|30/12/2022|1     |6           |2            |55      |[ENER, PHYS]            |\n",
      "|202200006|30/12/2022|1     |6           |17           |141     |[ENER, PHYS]            |\n",
      "|202200007|30/12/2022|1     |4           |18           |172     |[ENER, PHYS]            |\n",
      "|202200008|29/12/2022|1     |6           |0            |110     |[ENGI]                  |\n",
      "|202200009|29/12/2022|1     |1           |0            |23      |[SOCI]                  |\n",
      "|202200010|28/12/2022|1     |8           |1            |167     |[HEAL, MEDI]            |\n",
      "|202200011|27/12/2022|1     |23          |8            |54      |[MEDI]                  |\n",
      "|202200012|27/12/2022|1     |8           |8            |NULL    |[MEDI, IMMU]            |\n",
      "|202200013|27/12/2022|1     |8           |84           |217     |[MATE, ENGI, PHYS]      |\n",
      "|202200014|27/12/2022|1     |5           |10           |97      |[MEDI]                  |\n",
      "|202200015|23/12/2022|1     |9           |2            |60      |[VETE]                  |\n",
      "|202200016|23/12/2022|1     |2           |3            |56      |[SOCI]                  |\n",
      "|202200017|22/12/2022|1     |1           |NULL         |NULL    |[SOCI]                  |\n",
      "|202200018|22/12/2022|1     |1           |1            |53      |[SOCI]                  |\n",
      "|202200019|21/12/2022|1     |4           |1            |64      |[EART]                  |\n",
      "+---------+----------+------+------------+-------------+--------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/2_/rpnwqb4s7_3f6bmmnvl2mzj40000gn/T/ipykernel_13777/1777905033.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = df.withColumn(\"Class\", split(col(\"Class\"), \",\\s*\"))  # Split into array\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, regexp_replace\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DSDE2024\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read.option(\"header\", \"true\").csv('./2022data.csv')\n",
    "\n",
    "# Remove brackets and single quotes, then split into array\n",
    "df = df.withColumn(\"Class\", regexp_replace(col(\"Class\"), \"[\\\\[\\\\]'']\", \"\"))  # Remove [ ], and '\n",
    "df = df.withColumn(\"Class\", split(col(\"Class\"), \",\\s*\"))  # Split into array\n",
    "\n",
    "# Show the transformed DataFrame to verify\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "63978397-377c-4c00-a192-19ce1f8b16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3e82b34f-1dfe-45a3-ad78-7296f28ea493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3452"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f8f99cec-ab8c-48f3-bde5-35ab6bcc2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast type accordingly\n",
    "df = df.withColumn('citationCount', df.citationCount.cast('int'))\n",
    "df = df.withColumn('coAuthorship', df.coAuthorship.cast('int'))\n",
    "df = df.withColumn('refCount', df.refCount.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2bed7bef-0f01-4c98-bbd0-17ba00d663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, min, max, countDistinct, explode, split, col, round, sum\n",
    "\n",
    "max_values = df.agg(\n",
    "    max(\"citationCount\").alias(\"maxCitation\"),\n",
    "    max(\"refCount\").alias(\"maxRef\"),\n",
    "    max(\"coAuthorship\").alias(\"maxCoAuthor\")\n",
    ").collect()[0]\n",
    "\n",
    "# max value for each feature for normalization\n",
    "max_citation = max_values[\"maxCitation\"]\n",
    "max_ref = max_values[\"maxRef\"]\n",
    "max_coauthor = max_values[\"maxCoAuthor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cff27930-d46a-431e-ba8c-9432ad8a4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all the class: 27\n",
      "+-----+-----+\n",
      "|Genre|count|\n",
      "+-----+-----+\n",
      "| COMP|  201|\n",
      "| MATE|  284|\n",
      "| IMMU|  231|\n",
      "| ARTS|   70|\n",
      "| PHYS|  295|\n",
      "| HEAL|   49|\n",
      "| PSYC|   44|\n",
      "| BIOC|  435|\n",
      "| NEUR|   82|\n",
      "| VETE|  132|\n",
      "| ENGI|  394|\n",
      "| PHAR|  189|\n",
      "| MEDI| 1059|\n",
      "| ECON|   52|\n",
      "| MATH|   83|\n",
      "| MULT|  302|\n",
      "| ENVI|  332|\n",
      "| DECI|   18|\n",
      "| AGRI|  359|\n",
      "| ENER|  213|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find all Class\n",
    "genre_counts = df.withColumn(\"Genre\", explode(col(\"Class\")))\\\n",
    "                 .groupBy(\"Genre\")\\\n",
    "                 .count()  \n",
    "\n",
    "print('number of all the class:', genre_counts.count())\n",
    "genre_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "424773a6-1b80-49e0-a831-9dfcb0299c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the class\n",
    "exploded_df = df.withColumn(\"Class\", explode(col(\"Class\")))\n",
    "# exploded_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "027438d6-93ac-471f-ba68-f534a0713f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5867\n",
      "+---------+----------+------+------------+-------------+--------+-----+\n",
      "|       id|publicDate|source|coAuthorship|citationCount|refCount|Class|\n",
      "+---------+----------+------+------------+-------------+--------+-----+\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| BIOC|\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| CENG|\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| ENVI|\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| ENGI|\n",
      "|202200001|31/12/2022|     1|           7|            5|      39| BIOC|\n",
      "+---------+----------+------+------------+-------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop na for invalid rows\n",
    "cleaned_df = exploded_df.dropna()\n",
    "print(cleaned_df.count())\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "037084a9-ac9e-4954-8709-c6e6a647bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+------------+-------------+--------+-----+------+\n",
      "|       id|publicDate|source|coAuthorship|citationCount|refCount|Class| Score|\n",
      "+---------+----------+------+------------+-------------+--------+-----+------+\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| BIOC|0.1713|\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| CENG|0.1713|\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| ENVI|0.1713|\n",
      "|202200000|31/12/2022|     1|           3|            2|      44| ENGI|0.1713|\n",
      "|202200001|31/12/2022|     1|           7|            5|      39| BIOC|0.1881|\n",
      "|202200002|31/12/2022|     1|           6|            7|      51| BIOC|0.2497|\n",
      "|202200003|31/12/2022|     1|           5|           14|      50| CHEM|0.3207|\n",
      "|202200003|31/12/2022|     1|           5|           14|      50| CENG|0.3207|\n",
      "|202200003|31/12/2022|     1|           5|           14|      50| ENGI|0.3207|\n",
      "|202200004|30/12/2022|     1|           6|            2|      65| PHYS|0.2435|\n",
      "+---------+----------+------+------------+-------------+--------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the score for each paper\n",
    "cleaned_df = cleaned_df.withColumn(\n",
    "    \"Score\",\n",
    "    round(\n",
    "        col(\"source\") * (\n",
    "            0.4 * (col(\"citationCount\") / max_citation * 10) +\n",
    "            0.2 * (col(\"refCount\") / max_ref * 10) +\n",
    "            0.1 * (col(\"coAuthorship\") / max_coauthor * 10)\n",
    "        ), 4\n",
    "    )\n",
    ")\n",
    "\n",
    "cleaned_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bcb2e9eb-c9ae-4a35-8096-fb7ebe15ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+-----------+-----------+\n",
      "|Class|Year|Quarter|Total Score|Paper Count|\n",
      "+-----+----+-------+-----------+-----------+\n",
      "| BUSI|2022|      2|     4.3313|         17|\n",
      "| BUSI|2022|      4|     3.9284|         16|\n",
      "| MATH|2022|      2|     4.3977|         17|\n",
      "| COMP|2022|      2|    10.5223|         43|\n",
      "| EART|2022|      1|     8.9906|         23|\n",
      "| ECON|2022|      4|     3.3603|         14|\n",
      "| PSYC|2022|      1|     4.3177|         13|\n",
      "| PHYS|2022|      2|    46.9506|         78|\n",
      "| EART|2022|      4|     5.0058|         20|\n",
      "| PSYC|2022|      2|     2.5913|          8|\n",
      "| DENT|2022|      3|     2.4832|         16|\n",
      "| PHAR|2022|      2|    13.8496|         50|\n",
      "| DENT|2022|      1|     2.8371|         13|\n",
      "| COMP|2022|      1|     11.558|         35|\n",
      "| DECI|2022|      2|     0.1027|          1|\n",
      "| ENER|2022|      1|    27.0945|         50|\n",
      "| ENVI|2022|      2|    20.7589|         74|\n",
      "| COMP|2022|      4|    13.0639|         62|\n",
      "| EART|2022|      2|     5.1816|         19|\n",
      "| PHYS|2022|      4|    31.1773|         78|\n",
      "+-----+----+-------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, quarter, col, sum as spark_sum, round as spark_round, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming SparkSession is already created and cleaned_df is preloaded DataFrame\n",
    "# Convert publicDate from string to date type\n",
    "cleaned_df = cleaned_df.withColumn(\"publicDate\", to_date(col(\"publicDate\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Extract Year and Quarter from publicDate\n",
    "cleaned_df = cleaned_df.withColumn(\"Year\", year(col(\"publicDate\")))\n",
    "cleaned_df = cleaned_df.withColumn(\"Quarter\", quarter(col(\"publicDate\")))\n",
    "\n",
    "# Group by Class, Year, and Quarter and perform aggregations\n",
    "grouped_df = cleaned_df.groupBy(\"Class\", \"Year\", \"Quarter\").agg(\n",
    "    spark_round(spark_sum(\"Score\"), 4).alias(\"Total Score\"),\n",
    "    count(\"id\").alias(\"Paper Count\")  # Count the number of papers per group\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "710bfb81-f847-4f13-b777-96dafbec95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce the DataFrame to 1 partition to avoid multiple part files\n",
    "grouped_df.coalesce(1).write.csv(path=\"./test_output.csv\", mode=\"overwrite\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
