{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e79b6d-5de9-48f4-a71e-b96dd6b04ff4",
   "metadata": {},
   "source": [
    "# Install lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e40d6c5-1c97-40a7-b7af-05bdbea703fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2616d199-76ab-45c4-8a20-3c320e0783b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to print the json with the indent=4\n",
    "def jprint(data):\n",
    "  print(json.dumps(data,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6c86c-9126-4d3c-9c99-c66d7741b0bb",
   "metadata": {},
   "source": [
    "# Spark for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18935d5-f30d-452c-9423-53d9e84b17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 15:49:58 WARN Utils: Your hostname, Pirayans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.203.105.253 instead (on interface en0)\n",
      "24/05/09 15:49:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/09 15:49:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+------------+-------------+--------+------+\n",
      "|id   |publicDate|source|coAuthorship|citationCount|refCount|Class |\n",
      "+-----+----------+------+------------+-------------+--------+------+\n",
      "|N0001|14/12/2020|1.5   |19          |2            |44      |[MEDI]|\n",
      "|N0002|25/11/2020|1.5   |14          |5            |37      |[MEDI]|\n",
      "|N0003|09/12/2020|1.5   |29          |5            |44      |[MEDI]|\n",
      "|N0004|26/10/2020|1.5   |6           |1            |23      |[MEDI]|\n",
      "|N0005|16/11/2020|1.5   |12          |5            |53      |[MEDI]|\n",
      "|N0006|23/12/2020|1.5   |18          |5            |52      |[MEDI]|\n",
      "|N0007|15/12/2020|1.5   |11          |5            |70      |[MEDI]|\n",
      "|N0008|01/12/2020|1.5   |5           |3            |38      |[MEDI]|\n",
      "|N0009|21/10/2020|1.5   |20          |5            |43      |[MEDI]|\n",
      "|N0010|14/12/2020|1.5   |9           |33           |29      |[MEDI]|\n",
      "|N0011|21/12/2018|1.5   |14          |5            |54      |[BIOC]|\n",
      "|N0012|21/12/2018|1.5   |3           |33           |50      |[BIOC]|\n",
      "|N0013|31/12/2018|1.5   |7           |5            |61      |[BIOC]|\n",
      "|N0014|30/04/2018|1.5   |8           |5            |29      |[BIOC]|\n",
      "|N0015|03/12/2018|1.5   |14          |5            |47      |[BIOC]|\n",
      "|N0016|21/12/2018|1.5   |5           |4            |52      |[BIOC]|\n",
      "|N0017|04/12/2018|1.5   |10          |5            |49      |[BIOC]|\n",
      "|N0018|05/11/2018|1.5   |10          |5            |36      |[BIOC]|\n",
      "|N0019|17/12/2018|1.5   |7           |5            |71      |[BIOC]|\n",
      "|N0020|31/12/2018|1.5   |8           |5            |62      |[BIOC]|\n",
      "+-----+----------+------+------------+-------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, regexp_replace\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DSDE2024\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read.option(\"header\", \"true\").csv('./output.csv')\n",
    "\n",
    "# Remove brackets and single quotes, then split into array\n",
    "df = df.withColumn(\"Class\", regexp_replace(col(\"Class\"), \"[\\\\[\\\\]'']\", \"\"))  # Remove [ ], and '\n",
    "df = df.withColumn(\"Class\", split(col(\"Class\"), \",\\s*\"))  # Split into array\n",
    "\n",
    "# Show the transformed DataFrame to verify\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63978397-377c-4c00-a192-19ce1f8b16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e82b34f-1dfe-45a3-ad78-7296f28ea493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 15:50:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f99cec-ab8c-48f3-bde5-35ab6bcc2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast type accordingly\n",
    "df = df.withColumn('citationCount', df.citationCount.cast('int'))\n",
    "df = df.withColumn('coAuthorship', df.coAuthorship.cast('int'))\n",
    "df = df.withColumn('refCount', df.refCount.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bed7bef-0f01-4c98-bbd0-17ba00d663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, min, max, countDistinct, explode, split, col, round, sum\n",
    "\n",
    "max_values = df.agg(\n",
    "    max(\"citationCount\").alias(\"maxCitation\"),\n",
    "    max(\"refCount\").alias(\"maxRef\"),\n",
    "    max(\"coAuthorship\").alias(\"maxCoAuthor\")\n",
    ").collect()[0]\n",
    "\n",
    "# max value for each feature for normalization\n",
    "max_citation = max_values[\"maxCitation\"]\n",
    "max_ref = max_values[\"maxRef\"]\n",
    "max_coauthor = max_values[\"maxCoAuthor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff27930-d46a-431e-ba8c-9432ad8a4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all the class: 7\n",
      "+-----+-----+\n",
      "|Genre|count|\n",
      "+-----+-----+\n",
      "| COMP|   60|\n",
      "| IMMU|   60|\n",
      "| PHYS|   60|\n",
      "| BIOC|  120|\n",
      "| NEUR|   60|\n",
      "| MEDI|  780|\n",
      "| CHEM|   60|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find all Class\n",
    "genre_counts = df.withColumn(\"Genre\", explode(col(\"Class\")))\\\n",
    "                 .groupBy(\"Genre\")\\\n",
    "                 .count()  \n",
    "\n",
    "print('number of all the class:', genre_counts.count())\n",
    "genre_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424773a6-1b80-49e0-a831-9dfcb0299c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the class\n",
    "exploded_df = df.withColumn(\"Class\", explode(col(\"Class\")))\n",
    "# exploded_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027438d6-93ac-471f-ba68-f534a0713f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "+-----+----------+------+------------+-------------+--------+-----+\n",
      "|   id|publicDate|source|coAuthorship|citationCount|refCount|Class|\n",
      "+-----+----------+------+------------+-------------+--------+-----+\n",
      "|N0001|14/12/2020|   1.5|          19|            2|      44| MEDI|\n",
      "|N0002|25/11/2020|   1.5|          14|            5|      37| MEDI|\n",
      "|N0003|09/12/2020|   1.5|          29|            5|      44| MEDI|\n",
      "|N0004|26/10/2020|   1.5|           6|            1|      23| MEDI|\n",
      "|N0005|16/11/2020|   1.5|          12|            5|      53| MEDI|\n",
      "+-----+----------+------+------------+-------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop na for invalid rows\n",
    "cleaned_df = exploded_df.dropna()\n",
    "print(cleaned_df.count())\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037084a9-ac9e-4954-8709-c6e6a647bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+------------+-------------+--------+-----+------+\n",
      "|   id|publicDate|source|coAuthorship|citationCount|refCount|Class| Score|\n",
      "+-----+----------+------+------------+-------------+--------+-----+------+\n",
      "|N0001|14/12/2020|   1.5|          19|            2|      44| MEDI|1.2426|\n",
      "|N0002|25/11/2020|   1.5|          14|            5|      37| MEDI|1.6426|\n",
      "|N0003|09/12/2020|   1.5|          29|            5|      44| MEDI|1.8165|\n",
      "|N0004|26/10/2020|   1.5|           6|            1|      23| MEDI|0.6301|\n",
      "|N0005|16/11/2020|   1.5|          12|            5|      53| MEDI|1.9369|\n",
      "|N0006|23/12/2020|   1.5|          18|            5|      52| MEDI|1.9352|\n",
      "|N0007|15/12/2020|   1.5|          11|            5|      70| MEDI|2.2528|\n",
      "|N0008|01/12/2020|   1.5|           5|            3|      38| MEDI|1.2722|\n",
      "|N0009|21/10/2020|   1.5|          20|            5|      43| MEDI|1.7722|\n",
      "|N0010|14/12/2020|   1.5|           9|           33|      29| MEDI|6.5693|\n",
      "+-----+----------+------+------------+-------------+--------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the score for each paper\n",
    "cleaned_df = cleaned_df.withColumn(\n",
    "    \"Score\",\n",
    "    round(\n",
    "        col(\"source\") * (\n",
    "            0.4 * (col(\"citationCount\") / max_citation * 10) +\n",
    "            0.2 * (col(\"refCount\") / max_ref * 10) +\n",
    "            0.1 * (col(\"coAuthorship\") / max_coauthor * 10)\n",
    "        ), 4\n",
    "    )\n",
    ")\n",
    "\n",
    "cleaned_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb2e9eb-c9ae-4a35-8096-fb7ebe15ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+-----------+-----------+\n",
      "|Class|Year|Quarter|Total Score|Paper Count|\n",
      "+-----+----+-------+-----------+-----------+\n",
      "| IMMU|2019|      3|     2.3568|          1|\n",
      "| NEUR|2020|      2|     1.8807|          1|\n",
      "| MEDI|2019|      4|   270.1514|        122|\n",
      "| PHYS|2018|      4|    23.7444|         10|\n",
      "| MEDI|2023|      3|      1.208|          2|\n",
      "| PHYS|2023|      1|     7.0614|          1|\n",
      "| CHEM|2021|      4|    30.0819|          9|\n",
      "| MEDI|2021|      2|      4.196|          3|\n",
      "| COMP|2022|      4|    32.7025|         10|\n",
      "| MEDI|2023|      4|   640.2262|        117|\n",
      "| PHYS|2022|      4|    34.0808|         10|\n",
      "| NEUR|2021|      4|    32.2437|         10|\n",
      "| COMP|2020|      4|    22.6319|          8|\n",
      "| IMMU|2022|      3|     7.7949|          1|\n",
      "| BIOC|2019|      4|     37.203|         18|\n",
      "| MEDI|2023|      2|    11.4988|          8|\n",
      "| IMMU|2023|      4|    48.8768|         10|\n",
      "| BIOC|2023|      4|    87.5925|         17|\n",
      "| NEUR|2023|      2|      1.808|          1|\n",
      "| IMMU|2018|      4|    19.7245|         10|\n",
      "+-----+----+-------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, quarter, col, sum as spark_sum, round as spark_round, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming SparkSession is already created and cleaned_df is preloaded DataFrame\n",
    "# Convert publicDate from string to date type\n",
    "cleaned_df = cleaned_df.withColumn(\"publicDate\", to_date(col(\"publicDate\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Extract Year and Quarter from publicDate\n",
    "cleaned_df = cleaned_df.withColumn(\"Year\", year(col(\"publicDate\")))\n",
    "cleaned_df = cleaned_df.withColumn(\"Quarter\", quarter(col(\"publicDate\")))\n",
    "\n",
    "# Group by Class, Year, and Quarter and perform aggregations\n",
    "grouped_df = cleaned_df.groupBy(\"Class\", \"Year\", \"Quarter\").agg(\n",
    "    spark_round(spark_sum(\"Score\"), 4).alias(\"Total Score\"),\n",
    "    count(\"id\").alias(\"Paper Count\")  # Count the number of papers per group\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710bfb81-f847-4f13-b777-96dafbec95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce the DataFrame to 1 partition to avoid multiple part files\n",
    "grouped_df.coalesce(1).write.csv(path=\"./nauture.csv\", mode=\"overwrite\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
